"""
Base Employee Class
All AI employees inherit from this class
"""

import os
import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, AsyncIterator

from anthropic import AsyncAnthropic
from pydantic import BaseModel

from ..memory import MemoryManager

logger = logging.getLogger(__name__)


class Artifact(BaseModel):
    """Represents an artifact generated by an employee"""
    type: str  # code, diagram, document, checklist
    title: str
    content: str
    language: Optional[str] = None


class ChatResponse(BaseModel):
    """Response from an employee chat"""
    message: str
    suggestions: List[str] = []
    artifacts: List[Artifact] = []
    confidence: float = 0.9
    tokens_used: int = 0


@dataclass
class ChatContext:
    """Context for a chat session"""
    project_id: str
    user_id: str
    messages: List[Dict[str, Any]] = field(default_factory=list)
    metadata: Dict[str, str] = field(default_factory=dict)


class BaseEmployee(ABC):
    """Base class for all AI employees"""

    _client: Optional[AsyncAnthropic] = None
    _memory_managers: Dict[str, MemoryManager] = {}  # Cache memory managers by employee_id

    def __init__(self):
        # Client is created lazily to ensure env is loaded
        pass

    @property
    def memory_enabled(self) -> bool:
        """Whether memory is enabled for this employee"""
        return os.getenv("MEMORY_ENABLED", "true").lower() == "true"

    def get_memory_manager(self) -> MemoryManager:
        """Get or create memory manager for this employee"""
        if self.id not in BaseEmployee._memory_managers:
            BaseEmployee._memory_managers[self.id] = MemoryManager(self.id)
        return BaseEmployee._memory_managers[self.id]

    @property
    def tools(self) -> List[Dict[str, Any]]:
        """
        Tools available to this employee.
        All employees have web_search capability by default.
        Override in subclasses to add more tools.
        """
        return [
            {
                "type": "web_search_20250305",
                "name": "web_search",
                "max_uses": 3,  # 每次对话最多3次搜索
            }
        ]

    @property
    def client(self) -> AsyncAnthropic:
        """Lazy initialization of Anthropic client"""
        if BaseEmployee._client is None:
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                raise ValueError("ANTHROPIC_API_KEY environment variable not set")
            BaseEmployee._client = AsyncAnthropic(api_key=api_key)
        return BaseEmployee._client

    @property
    def model(self) -> str:
        """AI model to use"""
        return os.getenv("AI_MODEL", "claude-sonnet-4-20250514")

    @property
    def max_tokens(self) -> int:
        """Maximum tokens for response"""
        return int(os.getenv("AI_MAX_TOKENS", "4096"))

    @property
    @abstractmethod
    def id(self) -> str:
        """Unique identifier for the employee"""
        pass

    @property
    @abstractmethod
    def name(self) -> str:
        """Full name of the employee"""
        pass

    @property
    @abstractmethod
    def title(self) -> str:
        """Job title"""
        pass

    @property
    @abstractmethod
    def department(self) -> str:
        """Department name"""
        pass

    @property
    def avatar(self) -> str:
        """Avatar URL or emoji"""
        return f"/avatars/{self.id}.png"

    @property
    @abstractmethod
    def description(self) -> str:
        """Short description of the employee"""
        pass

    @property
    @abstractmethod
    def capabilities(self) -> List[str]:
        """List of capabilities"""
        pass

    @property
    @abstractmethod
    def specialties(self) -> List[str]:
        """List of specialties"""
        pass

    @property
    @abstractmethod
    def personality(self) -> str:
        """Personality description"""
        pass

    @property
    @abstractmethod
    def system_prompt(self) -> str:
        """System prompt for the AI"""
        pass

    def _build_messages(self, user_message: str, context: ChatContext, memory_context: str = "") -> List[Dict[str, str]]:
        """Build the messages array for the API call"""
        messages = []

        # Add context messages
        for msg in context.messages:
            role = "user" if msg.get("role") == "user" else "assistant"
            messages.append({
                "role": role,
                "content": msg.get("content", "")
            })

        # Add the new user message with optional memory context
        if memory_context:
            # Format memory context prominently to ensure it's used
            user_content = f"""{memory_context}

---

USER'S CURRENT QUESTION:
{user_message}

IMPORTANT: When answering, you MUST reference and use the context above if it's relevant to the user's question. Mention specific details like names, technologies, or preferences that were shared before."""
        else:
            user_content = user_message

        messages.append({
            "role": "user",
            "content": user_content
        })

        return messages

    async def chat(self, message: str, context: ChatContext) -> ChatResponse:
        """
        Chat with the employee

        Args:
            message: The user's message
            context: Chat context including history and metadata

        Returns:
            ChatResponse with the employee's response
        """
        try:
            # Retrieve relevant memories
            memory_context = ""
            if self.memory_enabled:
                try:
                    memory_manager = self.get_memory_manager()
                    memory_context = await memory_manager.get_context_for_chat(
                        message, context.project_id, max_memories=3
                    )
                    if memory_context:
                        logger.debug(f"Retrieved memory context for {self.id}")
                except Exception as e:
                    logger.warning(f"Failed to retrieve memories: {e}")

            messages = self._build_messages(message, context, memory_context)

            # Build API call parameters
            api_params = {
                "model": self.model,
                "max_tokens": self.max_tokens,
                "system": self.system_prompt,
                "messages": messages
            }

            # Add tools if available
            if self.tools:
                api_params["tools"] = self.tools
                # Check if web_search tool is included - requires beta header
                has_web_search = any(t.get("type", "").startswith("web_search") for t in self.tools)
                if has_web_search:
                    api_params["extra_headers"] = {"anthropic-beta": "web-search-2025-03-05"}

            response = await self.client.messages.create(**api_params)

            # Handle tool use loop (for web_search etc.)
            total_tokens = response.usage.input_tokens + response.usage.output_tokens
            max_tool_iterations = 10  # Prevent infinite loops
            iteration = 0

            while response.stop_reason == "tool_use" and iteration < max_tool_iterations:
                iteration += 1
                # Process tool calls
                assistant_content = []
                tool_results = []

                for block in response.content:
                    if block.type == "tool_use":
                        logger.info(f"Tool use: {block.name} with input: {block.input}")
                        assistant_content.append({
                            "type": "tool_use",
                            "id": block.id,
                            "name": block.name,
                            "input": block.input
                        })
                        # For web_search, the result is handled server-side
                        # We just need to acknowledge the tool use
                        tool_results.append({
                            "type": "tool_result",
                            "tool_use_id": block.id,
                            "content": "Search completed"  # Placeholder for server-side tools
                        })
                    elif block.type == "text":
                        assistant_content.append({
                            "type": "text",
                            "text": block.text
                        })

                # Add assistant message and tool results
                messages.append({"role": "assistant", "content": assistant_content})
                if tool_results:
                    messages.append({"role": "user", "content": tool_results})

                # Continue the conversation
                response = await self.client.messages.create(**api_params)
                total_tokens += response.usage.input_tokens + response.usage.output_tokens

            # Extract text content
            text_content = ""
            for block in response.content:
                if block.type == "text":
                    text_content += block.text

            # Parse artifacts from response
            artifacts = self._parse_artifacts(text_content)

            # Generate suggestions
            suggestions = self._generate_suggestions(text_content, context)

            # Save conversation to memory
            if self.memory_enabled and text_content:
                try:
                    memory_manager = self.get_memory_manager()
                    await memory_manager.save(
                        message=message,
                        response=text_content,
                        project_id=context.project_id,
                        metadata={"user_id": context.user_id}
                    )
                    logger.debug(f"Saved memory for {self.id}")
                except Exception as e:
                    logger.warning(f"Failed to save memory: {e}")

            return ChatResponse(
                message=text_content,
                suggestions=suggestions,
                artifacts=artifacts,
                confidence=0.9,
                tokens_used=total_tokens
            )

        except Exception as e:
            logger.error(f"Error in chat with {self.id}: {e}")
            raise

    async def chat_stream(self, message: str, context: ChatContext) -> AsyncIterator[str]:
        """
        Stream chat response

        Args:
            message: The user's message
            context: Chat context

        Yields:
            Text chunks as they are generated
        """
        try:
            # Retrieve relevant memories
            memory_context = ""
            if self.memory_enabled:
                try:
                    memory_manager = self.get_memory_manager()
                    memory_context = await memory_manager.get_context_for_chat(
                        message, context.project_id, max_memories=3
                    )
                except Exception as e:
                    logger.warning(f"Failed to retrieve memories for streaming: {e}")

            messages = self._build_messages(message, context, memory_context)

            # Track full response for memory saving
            full_response = ""

            # Build stream parameters
            stream_params = {
                "model": self.model,
                "max_tokens": self.max_tokens,
                "system": self.system_prompt,
                "messages": messages
            }

            # Add tools if available
            if self.tools:
                stream_params["tools"] = self.tools
                has_web_search = any(t.get("type", "").startswith("web_search") for t in self.tools)
                if has_web_search:
                    stream_params["extra_headers"] = {"anthropic-beta": "web-search-2025-03-05"}

            async with self.client.messages.stream(**stream_params) as stream:
                async for text in stream.text_stream:
                    full_response += text
                    yield text

            # Save conversation to memory after streaming completes
            if self.memory_enabled and full_response:
                try:
                    memory_manager = self.get_memory_manager()
                    await memory_manager.save(
                        message=message,
                        response=full_response,
                        project_id=context.project_id,
                        metadata={"user_id": context.user_id}
                    )
                    logger.debug(f"Saved streaming memory for {self.id}")
                except Exception as e:
                    logger.warning(f"Failed to save streaming memory: {e}")

        except Exception as e:
            logger.error(f"Error in chat_stream with {self.id}: {e}")
            raise

    def _parse_artifacts(self, text: str) -> List[Artifact]:
        """
        Parse artifacts from response text
        Looks for code blocks and other structured content
        """
        artifacts = []
        import re

        # Find code blocks
        code_pattern = r"```(\w+)?\n(.*?)```"
        matches = re.findall(code_pattern, text, re.DOTALL)

        for i, (language, code) in enumerate(matches):
            artifacts.append(Artifact(
                type="code",
                title=f"Code snippet {i + 1}",
                content=code.strip(),
                language=language or "text"
            ))

        return artifacts

    def _generate_suggestions(self, response: str, context: ChatContext) -> List[str]:
        """
        Generate follow-up suggestions based on the response
        Override in subclasses for custom suggestions
        """
        # Default suggestions - can be overridden by subclasses
        return [
            "Tell me more about this",
            "What are the next steps?",
            "Can you explain in more detail?"
        ]
